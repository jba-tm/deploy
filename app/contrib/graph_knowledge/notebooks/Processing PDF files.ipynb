{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a given PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            full_text = ''\n",
    "            for i in range(len(pdf.pages)):\n",
    "                page_text = pdf.pages[i].extract_text()\n",
    "                if page_text:\n",
    "                    full_text += page_text\n",
    "            return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zeyad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zeyad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = extract_text_from_pdf(\"\\\\\".join(os.path.dirname(os.path.abspath(\"__file__\")).split(\"\\\\\")[0:-1]) + \"\\\\data\\\\Unstructured data\\\\Azithromycin_tab_50730_RC1-08.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the extracted text.\n",
    "    - Removes special characters\n",
    "    - Normalizes whitespace\n",
    "    - Converts to lowercase (if necessary)\n",
    "    \"\"\"\n",
    "    \n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž .]\", re.IGNORECASE)\n",
    "    RE_WDOT = re.compile(r\"\\.+\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    \n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_WDOT, \".\", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "    text = text.replace(\" .\", \".\")\n",
    "    text = text.replace(\"..\", \".\")\n",
    "    # Convert to lowercase\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "    #words_filtered = [stemmer.stem(word) for word in words_tokens_lower if word not in stop_words]\n",
    "\n",
    "    text_clean = \" \".join(word_tokens)\n",
    "    return text_clean.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Contains Nonbinding Recommendations Draft Guidance On Azithromycin This Draft Guidance Once Finalized Will Represent The Food And Drug Administration Fda Current Thinking On This Topic .',\n",
       " 'It Does Not Create Or Confer Any Rights For Or On Any Person And Does Not Operate To Bind Fda Or The Public .',\n",
       " 'You Can Use An Alternative Approach If The Approach Satisfies The Requirements Of The Applicable Statutes And Regulations .',\n",
       " 'If You Want To Discuss An Alternative Approach Contact The Office Of Generic Drugs .',\n",
       " 'Active Ingredient Azithromycin Form Route Tablets Oral Recommended Studies Studies .',\n",
       " 'Type Of Study Fasting Design Single Dose Two Way Crossover In Vivo Strength Mg Subjects Normal Healthy Males And Females General Population .',\n",
       " 'Additional Comments .',\n",
       " 'Type Of Study Fed Design Single Dose Two Way Crossover In Vivo Strength Mg Subjects Normal Healthy Males And Females General Population .',\n",
       " 'Additional Comments Analytes To Measure Azithromycin Bioequivalence Based On Ci Azithromycin Waiver Request Of In Vivo Testing Mg And Mg Based On Acceptable Bioequivalence Studies On The Mg Strength Ii Proportionally Similar Across All Strengths And Iii Acceptable In Vitro Dissolution Testing Of All Strengths .',\n",
       " 'Since Azithromycin Tablets Mg Mg And Mg Are The Subject Of Three Separate New Drug Applications Nda Three Separate Abbreviated New Drug Applications Anda Must Be Submitted .',\n",
       " 'You May Request Waiver Of In Vivo Bioequivalence Testing Of The Mg And The Mg Strengths If You Meet The Criteria .',\n",
       " 'In Addition Please Cross Reference The In Vivo Bioequivalence Studies Conducted On The Higher Strength Along With Your Waiver Request .',\n",
       " 'Please Refer To The Guidance For Industry Variations In Drug Products That May Be Included In Single Anda Located At Http Www.Fda.Gov Cder Guidance .',\n",
       " 'Dissolution Test Method And Sampling Times Please Note That Dissolution Methods Database Is Available To The Public At The Ogd Website At Http Www.Fda.Gov Cder Ogd Index.Htm .',\n",
       " 'Please Find The Dissolution Information For This Product At This Website .',\n",
       " 'Please Conduct Comparative Dissolution Testing On Dosage Units Each Of All Strengths Of The Test And Reference Products .',\n",
       " 'Specifications Will Be Determined Upon Review Of The Application .',\n",
       " 'Recommended Jan']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = clean_text(full_text)\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities saved to c:\\Users\\zeyad\\Documents\\Clinical-Trials-Knowledge-Graph-Project\\data\\Structured data\\entities.parquet\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n",
    "\n",
    "\n",
    "entities_data = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    entities = pipe(sentence)\n",
    "    for entity in entities:\n",
    "        entity_data = {\n",
    "            'entity_group': entity['entity_group'],\n",
    "            'word': entity['word']\n",
    "        }\n",
    "        entities_data.append(entity_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(entities_data)\n",
    "\n",
    "\n",
    "\n",
    "def merge_tokens(df):\n",
    "    merged_data = []\n",
    "    current_word = ''\n",
    "    current_entity_group = None\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Check if the current row continues the current word\n",
    "        if row['entity_group'] == current_entity_group and (current_word.endswith('##') or row['word'].startswith('##')):\n",
    "            current_word += row['word'].lstrip('#')\n",
    "        else:\n",
    "            if current_word:\n",
    "                # Save the previous word before starting a new one\n",
    "                merged_data.append({'entity_group': current_entity_group, 'word': current_word})\n",
    "            current_word = row['word'].lstrip('#')\n",
    "            current_entity_group = row['entity_group']\n",
    "\n",
    "    # Don't forget to add the last word\n",
    "    if current_word:\n",
    "        merged_data.append({'entity_group': current_entity_group, 'word': current_word})\n",
    "\n",
    "    return pd.DataFrame(merged_data)\n",
    "\n",
    "\n",
    "df = merge_tokens(df)\n",
    "\n",
    "\n",
    "# Save to Parquet file\n",
    "parquet_file_path = \"\\\\\".join(os.path.dirname(os.path.abspath(\"__file__\")).split(\"\\\\\")[0:-1]) + \"\\\\data\\\\Structured data\\\\\"+'entities.parquet'\n",
    "df.to_parquet(parquet_file_path, engine = \"fastparquet\", index=False)\n",
    "\n",
    "print(f\"Entities saved to {parquet_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
